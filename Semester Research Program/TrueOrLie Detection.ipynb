{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b8cd3e",
   "metadata": {},
   "source": [
    "# 01. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d359e53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: tensorflow-gpu in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (2.8.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.5.5.64)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.8.9.1)\n",
      "Requirement already satisfied: sklearn in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow) (3.20.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.44.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mediapipe) (21.2.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\hp\\anaconda3\\lib\\site-packages (from mediapipe) (4.5.5.64)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow tensorflow-gpu opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2c91e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2                                # Computer Vison Library OpenCV\n",
    "import numpy as np                        # numpy arrays structure our datasets\n",
    "import os                                 # work with operating systems file path\n",
    "from matplotlib import pyplot as plt      # matplotlib just helps us visualize images a little bit easier\n",
    "import mediapipe as mp                    # mediapipe (https://google.github.io/mediapipe/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5edfdc7",
   "metadata": {},
   "source": [
    "# 02. Keypoints Using MP FashMesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7192d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh              # FaceMesh Model has -- 478 -- Keypoints \n",
    "mp_drawing_styles = mp.solutions.drawing_styles    # Drawing styles\n",
    "mp_drawing = mp.solutions.drawing_utils            # Drawing utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2884357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image,model):\n",
    "    image = cv2.cvtColor(image , cv2.COLOR_BGR2RGB)   # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                     # Image is no longer writeable\n",
    "    results = model.process(image)                    # Make prediction\n",
    "    image.flags.writeable = True                      # Image is now writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)    # COLOR CONVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15a78915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image,face_landmarks):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(\n",
    "            image= image,\n",
    "            landmark_list= face_landmarks,\n",
    "            connections= mp_face_mesh.FACEMESH_TESSELATION,\n",
    "            landmark_drawing_spec= None,\n",
    "            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "    \n",
    "    # Draw face countours (FACEMESH_CONTOURS = frozenset().union(*[FACEMESH_LIPS, FACEMESH_LEFT_EYE, FACEMESH_LEFT_EYEBROW, FACEME) connections\n",
    "    mp_drawing.draw_landmarks(\n",
    "            image= image,\n",
    "            landmark_list= face_landmarks,\n",
    "            connections= mp_face_mesh.FACEMESH_CONTOURS,\n",
    "            landmark_drawing_spec= None,\n",
    "            connection_drawing_spec= mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "    \n",
    "    # Draw eye iris ( FACEMESH_IRISES = frozenset().union(*[FACEMESH_LEFT_IRIS, FACEMESH_RIGHT_IRIS]) ) connections\n",
    "    mp_drawing.draw_landmarks(\n",
    "            image= image,\n",
    "            landmark_list= face_landmarks,\n",
    "            connections= mp_face_mesh.FACEMESH_IRISES,\n",
    "            landmark_drawing_spec= None,\n",
    "            connection_drawing_spec= mp_drawing_styles.get_default_face_mesh_iris_connections_style())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e2797",
   "metadata": {},
   "source": [
    "# 03. Extract Keypoints Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19337fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    face = np.array([[res.x , res.y , res.z] for res in results.multi_face_landmarks[0].landmark]).flatten() if results.multi_face_landmarks[0].landmark else np.zeros(478*3)\n",
    "    return np.concatenate([face])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b56b0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for res in results.multi_face_landmarks[0].landmark]).flatten():\n",
    "#    if results.multi_face_landmarks[0].landmark:\n",
    "#        np.array([[res.x , res.y , res.z]\n",
    "#    else :\n",
    "#      np.zeros(478*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d42a2a",
   "metadata": {},
   "source": [
    "# 04. Setup Folder for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6190fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('ML_Data')         # Path for exported data, numpy arrays\n",
    "\n",
    "actions = np.array(['true', 'lie'])       # Actions that we try to detect\n",
    "\n",
    "no_sequences = 2                       # videos worth of data per action (True-02 vdos, Lie-02 Vdos)\n",
    "\n",
    "sequence_length = 30              # Videos are going to be 30 frames in length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6898e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Folders for save keypoints values\n",
    "\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13042637",
   "metadata": {},
   "source": [
    "#  05. Collect Keypoints Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebd2b6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "____________Finish__________________\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/true/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "____________Finish__________________\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/0/0.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "____________Finish__________________\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "Processing Video Path: Datasets/lie/1/1.mp4\n",
      "Keypoints Shape: (1434,)\n",
      "____________Finish__________________\n"
     ]
    }
   ],
   "source": [
    "for action in actions:   # Loop through actions\n",
    "    \n",
    "    for sequence in range(no_sequences):    # Loop through sequences (videos)\n",
    "        \n",
    "        VDO_PATH = 'Datasets/' + action +'/' + str(sequence) + '/' + str(sequence) + '.mp4'  # /Datasets/true/0.mp4\n",
    "        cap = cv2.VideoCapture(VDO_PATH)   # Read Video\n",
    "        \n",
    "        with mp_face_mesh.FaceMesh(   # set mediapipe model\n",
    "            max_num_faces =1,\n",
    "            refine_landmarks = True,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as face_mesh:\n",
    "            for frame_num in range(sequence_length):\n",
    "                \n",
    "                ret, frame = cap.read()  # Read feed\n",
    "\n",
    "                print(\"Processing Video Path:\",VDO_PATH)          # Processing Video Path:Datasets/true/0/0.mp4\n",
    "\n",
    "                image, results = mediapipe_detection(frame, face_mesh)                        # Make detections\n",
    "\n",
    "#                 ### Draw landmarks ###\n",
    "#                 cv2.waitKey(1)\n",
    "#                 if results.multi_face_landmarks:\n",
    "#                     for face_landmarks in results.multi_face_landmarks:\n",
    "#                         draw_landmarks(image,face_landmarks)\n",
    "#                         cv2.imshow('OpenCV feed', image)\n",
    "\n",
    "                keypoints = extract_keypoints(results)                                       # Extract keypoints\n",
    "            \n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))     # Export keypoints\n",
    "                \n",
    "                print(\"Keypoints Shape:\",keypoints.shape)                              # Keypoints Shape:(1434,)\n",
    "                \n",
    "                np.save(npy_path, keypoints)                                                    # Save Keypoints\n",
    "\n",
    "\n",
    "            cap.release()     # Release feed\n",
    "            \n",
    "            cv2.waitKey(1)    # wait for 1 Secound\n",
    "            \n",
    "            print(\"____________Finish__________________\")                    # ____________Finish__________________\n",
    "            \n",
    "#     cv2.destroyAllWindows()   # Close All OpenCV Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbca549b",
   "metadata": {},
   "source": [
    "# 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42758aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # Scikit-learn for our evaluation metrics\n",
    "from tensorflow.keras.utils import to_categorical   # Converting Data (Encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0833bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}  # Actions Label Map  {'true': 0, 'lie': 1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a76bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []                # like : Features(X) Data , labels(Y) Data\n",
    "for action in actions:              \n",
    "    for sequence in range(no_sequences):  \n",
    "        window = []                       # Save all of diff.frames paricular sequence\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))  # load np array\n",
    "            window.append(res)                # Append all res \n",
    "        sequences.append(window)             # Append all window\n",
    "        labels.append(label_map[action])   # Append all actions on labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3893a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)  # sequences\n",
    "y = to_categorical(labels).astype(int)  # labels\n",
    "\n",
    "# Train & Testing Partition\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3a2b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape : (4, 30, 1434)  -----> ( Sequences(videos) , framers per vdo, keypoints shape [478 * 3] )\n",
    "# y.shape : (4, 2) -------->   ( Sequences(videos), labels[actions])\n",
    "\n",
    "# X_train.shape : (3, 30, 1434) \n",
    "# X_test.shape : (1, 30, 1434)\n",
    "\n",
    "# y_train.shape : (3, 2) \n",
    "# y_test.shape : (1, 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1cd08",
   "metadata": {},
   "source": [
    "# 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "600980f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "607e54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89b74c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1434)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "100f555f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "942cb039",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [.7, 0.2, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb94d5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cd8e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa7392da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step - loss: 0.7069 - categorical_accuracy: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27f3e7b9760>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=1, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0011ea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 30, 64)            383744    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 30, 128)           98816     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 538,274\n",
      "Trainable params: 538,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e4cdb",
   "metadata": {},
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a410a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8590617e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53625345, 0.46374655]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87439d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5bc6de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff9be214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10931ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb67dcb",
   "metadata": {},
   "source": [
    "# 9. Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0911c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3850a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdcff140",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5588/3615422507.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'action.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53974bd",
   "metadata": {},
   "source": [
    "# 10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f78a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3bfbbdb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5588/1543913239.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdda16b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yhat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5588/750581457.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mytrue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0myhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'yhat' is not defined"
     ]
    }
   ],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39afdccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa2ca635",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yhat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5588/477288958.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmultilabel_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'yhat' is not defined"
     ]
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47527c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1763f9b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'yhat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5588/3489675696.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'yhat' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb44098",
   "metadata": {},
   "source": [
    "# 11. Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac5a92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "438d921c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'SolutionOutputs' has no attribute 'landmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5588/3570539042.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Draw landmarks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mdraw_landmarks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# 2. Prediction logic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5588/1769022181.py\u001b[0m in \u001b[0;36mdraw_landmarks\u001b[1;34m(image, face_landmarks)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdraw_landmarks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mface_landmarks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Draw face connections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     mp_drawing.draw_landmarks(\n\u001b[0m\u001b[0;32m      4\u001b[0m             \u001b[0mimage\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mlandmark_list\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mface_landmarks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mediapipe\\python\\solutions\\drawing_utils.py\u001b[0m in \u001b[0;36mdraw_landmarks\u001b[1;34m(image, landmark_list, connections, landmark_drawing_spec, connection_drawing_spec)\u001b[0m\n\u001b[0;32m    157\u001b[0m   \u001b[0mimage_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m   \u001b[0midx_to_coordinates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m   \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlandmark\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlandmark_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m     if ((landmark.HasField('visibility') and\n\u001b[0;32m    161\u001b[0m          landmark.visibility < _VISIBILITY_THRESHOLD) or\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'SolutionOutputs' has no attribute 'landmark'"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.8\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, face_mesh)\n",
    "        print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "#         sequence.insert(0,keypoints)\n",
    "#         sequence = sequence[:30]\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            \n",
    "            \n",
    "        #3. Viz logic\n",
    "            if res[np.argmax(res)] > threshold: \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c1e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh.FaceMesh??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e41a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744718e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
